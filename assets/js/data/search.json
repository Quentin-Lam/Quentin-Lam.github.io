[ { "title": "如何安装Eyelink眼动实验所需的一切硬件", "url": "/posts/%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6/", "categories": "科研", "tags": "眼动", "date": "2023-08-02 05:14:00 +0000", "snippet": "Eyelink 1000 Plus 眼动仪是眼动研究中重要的研究设备。本文将从软件安装，硬件配置，到上手校准3个方面来帮助大家理解眼动实验的每个步骤。1. 软件安装首先，不管我们是用什么软件进行试验，我们都需要去Eyelink的官网(https://www.sr-research.com/support/)安装所需的组件，网页界面如下：注册完注意查收自己的邮箱点击链接完成验证。完成验证后，回...", "content": "Eyelink 1000 Plus 眼动仪是眼动研究中重要的研究设备。本文将从软件安装，硬件配置，到上手校准3个方面来帮助大家理解眼动实验的每个步骤。1. 软件安装首先，不管我们是用什么软件进行试验，我们都需要去Eyelink的官网(https://www.sr-research.com/support/)安装所需的组件，网页界面如下：注册完注意查收自己的邮箱点击链接完成验证。完成验证后，回到支援网站，会提示你已经验证成功。需要注意的是，注册完成后账户不会立马激活，可能要等一天才行。激活后会有邮件提示。注册完成后，我们回到下载界面（https://www.sr-research.com/support/），就能看到Eyelink的论坛被解锁啦~1.1 安装Psychopy因为我是用python编写的实验，所以这里我选择下载python相关的组件。Psychopy下载地址：https://www.psychopy.org/download.html 。安装完毕后再安装Visual Studio Code：https://code.visualstudio.com/Download 。之后回到Support Forum (https://www.sr-research.com/support/)，选择下载Developer Kit 和 Pylink Download。Pylink Download 可直接输入以下命令：py -3.8 -m pip install --index-url=https://pypi.sr-support.com sr-research-pylink# 安装显卡驱动呈现刺激(视情况安装)py -3.8 -m pip install pyopengl （如果安装的是其他版本的python就要把3.8替换为对应的版本数字）2. 实验代码编写2.1 Vscode python环境配置首先，我们在vscode中安装python拓展。在菜单右下角查看自己的python是否已经载入（我这里显示的版本是3.8.8）被试机（呈现刺激的电脑）硬件配置：记得，眼动仪——主试机，被试机——主试机要连两条网线，连好后要将被试机电脑的IP改成100.1.1，不知道怎么改可参考网站链接：https://www.sr-support.com/showthread.php?tid=281实验代码：首先，导入应该要导入的包并建立中文utf-8环境#!/usr/bin/env python# -*- coding: utf-8 -*-import osimport sys# math is for calculating visual anglefrom math import pi, atan, hypotimport randomimport pylink # we should have installed it before# This is for using animation for calibration, If not needed, we can comment out it. This isn't a standard package from python, we need to cite where it is for python to find itsys.path.append('C:/Users/20191/Desktop/ldk/6 yuan/exp_code') # The path where my .py module file isfrom EyeLinkCoreGraphicsPsychoPyAnimatedTarget import EyeLinkCoreGraphicsPsychoPy# import psychopy for stimulation presentationfrom psychopy import visual, core, event, monitorsfrom psychopy.constants import STOPPED, PLAYING# for preparing the Host backdrop imagefrom PIL import Image# 切换输入法，没有这个包的话pip installimport pyautogui# Specify the font dictionary to override default font paths# copy your font file to the same directory of your exp fileimport pygletexp_dir = os.path.dirname(os.path.realpath(__file__))os.chdir(exp_dir)font_name = 'kaiu.ttf'pyglet.font.add_file(os.path.join(exp_dir, font_name))为了方便大家理解，EyeLinkCoreGraphicsPsychoPyAnimatedTarget就是这样一个.py文件然后，我们可以输入一些硬件方面的信息（需要根据刺激呈现屏幕调整，需手动测量），放在前面是为了方便以后修改。# set a few constantsscrWidth = 37.5 # measured in cmscrHeight = 30# The resolution of presenting screenscnSize = [1024, 768] pw=scnSize[0] # screen width measured by pixelsph=scnSize[1]viewDist = 66 # The viewing distance from chin rest to the screen (cm)# the formula to calculate visual anglesdeg2pix = int((scnSize[0] / 2.0) / (atan(scrWidth / 2.0 / viewDist) * 180.0 / pi)) # stim sizeV_w = 9 # measured in visual anglesV_h = round(V_w * 300 / 260) # make sure that different sizes of stim keeps the same shapepix_size = (V_w * deg2pix, V_h * deg2pix)print(f\"The the width of the stimulus is {V_w} and the height is {V_h}, and the pixel size of it is {pix_size}\")然后，我们可以设置一些眼动仪相关的参数。# eye-selection global optionsRIGHT_EYE = 1LEFT_EYE = 0BINOCULAR = 2# Initialize EyeLink and the Graphicseyelinktracker = pylink.EyeLink()if not eyelinktracker:\tprint (\"EL is None\")\tsys.exit()\"\"\" if not dummyMode: \ttk = pylink.EyeLink('100.1.1.1')else:\ttk = pylink.EyeLink(None) \"\"\"tk = pylink.EyeLink('100.1.1.1')如果眼动仪启动成功，我们便可以开始收集被试信息。# get subj infodef getSubjInfo(): pyautogui.keyDown('shift')\tpyautogui.keyUp('shift')\tID = str(input('Subject ID: ')) # # You can add and delete whatever you like\t# Name = raw_input('Subject Name: ')\t# Gender = raw_input('Subject Gender: ')\tAge = str(input('Age: '))\treturn [ID, Age]def init_data_file(SUBJ_INFO):\t\"\"\" define a function that initializes the data file and get sub_info\"\"\"\tif not os.path.exists('csv_data_DotFace'): \t\tos.mkdir('csv_data_DotFace') # join everything in the SUBJ_INFO seperated by \"_\"\tfile_name = 'csv_data_DotFace/' + \"_\".join(SUBJ_INFO) + '.csv'\twith open(file_name, 'w') as data_file:\t\theader = ['subj', 'age', 'trial', 'dot_pos', 'Lstim', 'Rstim', 'stim_st', 'stim_et', 'LoEp', 'cue_pos', 'key_press'] # this behavorial csv data should contain the info of every trial. Please add or delete things in it if needed\t\tdata_file.write('\\t'.join(header) + '\\n')\treturn data_file## initialize the data files.subjInfo = getSubjInfo()DATA_FILE = init_data_file(subjInfo)dataFolder = os.getcwd() + '/your_experiment_name/' # CHANGE IT ACCORDING TO YOU EXPERIMENT!if not os.path.exists(dataFolder): os.makedirs(dataFolder) # create data folder if it's not thereedfFileName = \"_\".join(subjInfo) + \".EDF\" # create .EDF file for storing eye-tracking datatk.openDataFile(edfFileName)现在，我们完成了数据文件的创造，我们可以设计实验设计矩阵，这里我们以点探测实验为例建立矩阵。# prepare stimscript_path = os.path.abspath(os.path.dirname(__file__))proj_root = os.sep.join(script_path.split(os.sep)[:-1])# The trial list needs 5 variables, equals to: # cue_pos: the position of fixation point. 1:left, 2:right, 3:center# LoEp: the location of the emotional picture. 1:left, 2:right# dot_pos: the position of the pressing dot. 1:left, 2:right # stim_e: the picture name of the emotional pictures# stim_n: the picture name of the neutral picturescue_pos = [1, 2, 3]LoEp = [1, 2]dot_pos = [1, 2]# Define functions for generating trial listsdef generate_trial_list(cue_pos, LoEp, dot_pos, stim_e, stim_n): trial_list_all = [] for i in cue_pos: \tfor j in LoEp: for k in dot_pos: trial_list = [] for l in range(len(stim_e)): list1 = [i, j, k, stim_e[l], stim_n[l]] trial_list.append(list1) trial_list_all.extend(trial_list) return trial_list_all# Get the filenames for emotional and neutral stimulid_dir_ExpF = os.path.join(proj_root, 'exp_stim/exp/f/')d_dir_ExpM = os.path.join(proj_root + 'exp_stim/exp/m/')d_dir_NF = os.path.join(proj_root, 'exp_stim/N/f/')d_dir_NM = os.path.join(proj_root, 'exp_stim/N/m/')d_dir_all = os.path.join(proj_root, 'exp_stim/all/')file_name_ExpF = os.listdir(d_dir_ExpF)file_name_ExpM = os.listdir(d_dir_ExpM)file_name_NF = os.listdir(d_dir_NF)file_name_NM = os.listdir(d_dir_NM)file_name_Exp = file_name_ExpF + file_name_ExpMfile_name_N = file_name_NF + file_name_NM# print(file_name_Exp) # ['FF17.bmp', 'FF23.BMP', 'HF10.BMP', 'HF61.bmp', 'SF16.bmp', 'SF38.BMP', 'FM3.BMP', 'FM36.bmp', 'HM102.bmp', 'HM17.BMP', 'SM18.bmp', 'SM41.BMP']# Generate the trial lists for different conditionstrial_list_rec = generate_trial_list(cue_pos, LoEp, dot_pos, file_name_Exp, file_name_N)# print(trial_list_rec[0], len(trial_list_rec)) # [1, 1, 1, 'FF17.bmp', 'NF15.bmp'], 144这个是本点探测范式试次的流程图：以上都设置完了，我们可以初始化屏幕并进入首次校准阶段。# setup a displaymon = monitors.Monitor('QL', width=scrWidth, distance=viewDist) # name the display with whatever you wantmon.setSizePix((scnSize[0], scnSize[1]))surf = visual.Window((scnSize[0], scnSize[1]), fullscr=True, monitor=mon, color=[0.5,0.5,0.5], units='pix') # set up full screen and the background colorevent.Mouse(visible=False) # hide the mouse# call the custom calibration routine \"EyeLinkCoreGraphicsPsychopy.py\", instead of the default routines that were implemented in SDL# Initiate the custon calibration routinegenv = EyeLinkCoreGraphicsPsychoPy(tk, surf)# # to use a movie clip as the calibration target#genv.calTarget = 'movie'#genv.movieTargetFile = 'starjumps100.avi'# to use a rotating checkerboard as the calibration targetgenv.calTarget = 'rotatingCheckerboard'# Open the graphics window to show custom calibration picpylink.openGraphicsEx(genv)# # If the previous doesn't works, use it#pylink.openGraphics()# Flush keyboardpylink.flushGetkeyQueue()# We need to put the tracker in offline mode before we change its configurationstk.setOfflineMode()#pylink.setTargetSize(int(0.5*deg2pix), int(0.15*deg2pix))#pylink.setCalibrationColors(black, gray)## Gets the display surface and sends a mesage to EDF file;# set up sample ratetk.sendCommand('sample_rate 500')# record the exp pixel coordinatestk.sendCommand(\"screen_pixel_coords = 0 0 %d %d\" % (pw-1, ph-1))# sent it to the eye-trackertk.sendMessage(\"DISPLAY_COORDS 0 0 %d %d\" % (pw-1, ph-1))# specify the calibration type, H3, HV3, HV5, HV13 (HV = horiztonal/vertical), tk.sendCommand(\"calibration_type = HV5\") # tk.setCalibrationType('HV9') also works, see the Pylink manual http://sr-research.jp/support/manual/EyeLink%20Programmers%20Guide.pdf# specify the proportion of subject display to calibrate/validate (OPTIONAL, useful for wide screen monitors)tk.sendCommand(\"calibration_area_proportion 0.75 0.7\")tk.sendCommand(\"validation_area_proportion 0.75 0.7\")# Set the tracker to parse Events using \"GAZE\" (or \"HREF\") data#tk.sendCommand(\"recording_parse_type = GAZE\")# online parsing configuration, 0 for ‘cognitive’, 1 for psychophysical, which is more sensitive to saccadetk.sendCommand(\"select_parser_configuration 0\")# set up the viewing dist for eye-trackertk.sendCommand(format(\"simulation_screen_distance {viewDist}\"))# edf contentstk.sendCommand(\"file_sample_data = LEFT,RIGHT,GAZE,AREA,GAZERES,STATUS,HTARGET\")# set link data (used for gaze cursor) tk.sendCommand(\"link_event_filter = LEFT,RIGHT,FIXATION,FIXUPDATE,SACCADE,BLINK,BUTTON\")tk.sendCommand(\"link_sample_data = LEFT,RIGHT,GAZE,GAZERES,AREA,STATUS,HTARGET\")过完校准，我们就可以开始实验的练习阶段了。我们可以先打包一些基础的刺激绘制函数，方便我们之后呈现刺激。## pack some basic psychopy function to shorten themdef display_clear():\t\"\"\" clear up the screen \"\"\"\tsurf.color=[0.5,0.5,0.5]\tsurf.flip()def put_txt(txt, loc):\t\"\"\" Put text on display \"\"\"\ttext_1 = visual.TextStim(surf, text = txt,\tpos = loc,\tcolor = 'black',\tbold = True)\ttext_1.draw()\tsurf.flip() def draw_pic(stimuli, pos): img = visual.ImageStim(surf, image = stimuli, units = 'pix', size = (V_w, V_h), pos = pos) img.draw() # we don't flip here in case we need to present more pics at a time def draw_dot(color_arr, pos): dot = visual.Circle(surf, radius = deg2pix/2, units = 'pix', fillColor = color_arr, pos = pos) dot.draw() surf.flip() def driftCheck4(fix_pos, fixDur=500, acceptableDev=1*deg2pix): '''This is an optional function. It's to make sure that the trial only starts when the eyes gazed at the cue dot for long enough time (&gt;= fixDur). Use draw_dot if you don't need this. ''' # fixDotX, fixDotY are the position of the fixation dot event.clearEvents() # clear cached (keyboard/mouse etc.) events, if there is any # present the cue dot fix = draw_dot([0, 0, 0], fix_pos) fix.draw() surf.flip() triggered = False fixationStartTime = -1 # checks for the gaze position and calculates the gaze error in pixels from the specified fixation dot position while not triggered: # check if any new events are available dt = tk.getNextData() if dt &gt; 0: ev = tk.getFloatData() if dt == pylink.FIXUPDATE: # update the fixation start time if fixationStartTime &lt; 0: fixationStartTime = ev.getStartTime() # how much time has elapsed within the current fixation fixDuration = ev.getEndTime() - fixationStartTime # check if the average gaze position gazeX, gazeY = ev.getAverageGaze() fix.draw() # # comment out this if you want to show the gaze position # fix111 = visual.Circle(surf, radius=10, units='pix', fillColor=[1, 0, 0], pos=(gazeX - pw / 2, ph / 2 - gazeY)) # fix111.draw() surf.flip() gazeError = hypot((gazeX - pw / 2) - fix_pos[0], (ph / 2 - gazeY) - fix_pos[1]) # trigger if the duration of the fixation is &gt; 300 ms and # the gaze position is close to the fixation dot (&lt; 30 pixels) if fixDuration &gt;= fixDur and gazeError &lt; acceptableDev: triggered = True #tk.sendCommand('clear_screen 0') # clear the host screen # check for Escape key if event.getKeys(keyList=['escape']): # If the escape key is pressed, it initiates the eye-tracker setup and drift correction. tk.doTrackerSetup() #driftCheck(deg2pix, \"6.avi\") #tk.doDriftCorrect(int(fixDotX + pw / 2), int(ph / 2 - fixDotY), 1, 1) tk.doDriftCorrect(int(pw / 2), int(ph / 2), 1, 1) error = tk.startRecording(1, 1, 1, 1) if error: return error ## wait for 50 ms to prevent data loss pylink.msecDelay(40) tk.sendCommand('clear_screen 0') # clear the host screen def run_a_trial(subjInfo, trial_info, rec): # subjInfo include subnum, age .etc, trial_info is the conditions we have for each trial. rec: 0 equals learning phase, 1 equals experiment phase if rec == 1: # if it's the experiment phase, we should activate eye-tracker # force off-line mode first to prevent eyelink freeze tk.setOfflineMode() pylink.msecDelay(40) # let pylink delay 40ms to prevent data loss # get the info you need for running a trail cue_pos, LoEp, dot_pos, stim_e, stim_n = trial_info if cue_pos == 1: # cue_dot at the left cue_loc=L_rect elif cue_pos == 2: # cue_dot at the right cue_loc=R_rect else: # cue dot at the center cue_loc=(0,0) if LoEp == 1: e_loc = L_rect n_loc = R_rect else: e_loc = R_rect n_loc = L_rect if dot_pos == 1: dot_loc = L_rect else: dot_loc = R_rect if rec == 1: pylink.msecDelay(40) driftCheck4(cue_loc) tk.sendMessage(\"check ok\") msg = \"TRIALadd %d %d %d %d %s\" % (Ntrial, cue_pos, LoEp, dot_pos, stim_e) # The info sent to .edf corresponding to trial num, cue dot position, Location of emotional pic, pressing dot position, and emotional stim name img_e = draw_pic(os.path.join(d_dir_all, stim_e), pos = e_loc) img_n = draw_pic(os.path.join(d_dir_all, stim_n), pos = n_loc) else: # for the learning phase draw_dot([0, 0, 0], cue_loc) core.wait(2) draw_pic(os.path.join(d_dir_learn, stim_e), pos = e_loc) draw_pic(os.path.join(d_dir_learn, stim_n), pos = n_loc)\tsurf.flip() if rec == 1: img_st = tk.trackerTime() core.wait(2.5) if rec == 1: tk.sendMessage(\"trial ok1\") dot_st = core.getTime() draw_dot([0, 1, 0], dot_loc) resp = event.waitKeys(200000, key_list, timeStamped=True)\tif rec == 1: trial_data = subjInfo + [Ntrial] + trial_info + [img_st] + [dot_st] + list(resp[0]) elif rec == 0: trial_data = trial_info + list(resp[0])\tif (trial_info[2] == 1 and resp[0][0] == key_list[0]) | (trial_info[2] == 2 and resp[0][0] == key_list[1]): trial_data += [1] else: trial_data += [0]\tif rec == 0: learn_data.append(trial_data[-1])建立练习阶段：Ntrial = 0# This is for learning phased_dir_learn = os.path.join(proj_root, 'exp_stim/prac')stim_p1 = ['1.bmp']stim_p2 = ['2.bmp']LoEp_L = [1] # 两张练习图片是同质的我们就不换位置了learnlist = generate_trial_list(cue_pos, LoEp_L, dot_pos, stim_p1, stim_p2)# Here the L_rect means that the left pic goes 9 vis ang left away from the center of the screenL_rect=(-deg2pix * 9, 0) # lr_pos=1, rep leftR_rect=(deg2pix * 9, 0)def learn_phase(): # the loc (0,0) means the center of the screen\tput_txt(u\"\"\"現在請您練習下。 \\n\\n 白色小圓圈出現時，請看它。 \\n\\n 之後，兩張面孔出現，請自由觀察面孔。 \\n\\n 面孔消失後會出現綠色圓圈，請判斷在屏幕左邊還是右邊 \\n\\n 如果在左邊請按 z 鍵，在右邊按 / 鍵。 \\n\\n 明白後按空格鍵開始\"\"\", (0,0)) event.waitKeys(keyList =['space']) global key_list key_list = ['z', 'slash'] display_clear() random.shuffle(learnlist) global learn_data learn_data = [] for t1 in learnlist[:]: global Ntrial\t\tNtrial = Ntrial + 1\t\trun_a_trial(subjInfo, t1, 0)\t\tdisplay_clear() # the ITI is randomized between 1000 and 1400 ms\t\tcore.wait(random.uniform(1, 1.4)) learn_phase()acc = sum(learn_data) / len(learnlist) * 100put_txt(f\"很棒，你現在已經完成了練習階段，你的正確率是: \\n\\n {acc} %\", (0, 0))core.wait(1.5)" }, { "title": "【综述】A META-ANALYSIS OF THE MAGNITUDE OF BIASED ATTENTION IN DEPRESSION", "url": "/posts/Dot-probe-depression-review/", "categories": "文献", "tags": "抑郁", "date": "2022-09-04 10:50:00 +0000", "snippet": "作者：Andrew D. Peckham, B.A., R. Kathryn McHugh, M.A., and Michael W. Otto, Ph.D.年份:2010期刊:DEPRESSION AND ANXIETYDOI：10.1002/da.20755主要结果： 对比emotional stroop task和dot probe task的效应量发现，dot probe ...", "content": "作者：Andrew D. Peckham, B.A., R. Kathryn McHugh, M.A., and Michael W. Otto, Ph.D.年份:2010期刊:DEPRESSION AND ANXIETYDOI：10.1002/da.20755主要结果： 对比emotional stroop task和dot probe task的效应量发现，dot probe task更能反映出抑郁患者对消极情绪刺激的注意偏向（d = 0.52, P &lt; .001, 95% CI = .30, .74）对比emotional stroop的(d = 0.17, P = .06, 95% CI = .07, .34)。 Dot probe studies were categorized as using either verbal (emotional words, n = 5) or nonverbal (emotional faces or pictures, n = 9) stimuli (all Stroop studies used verbal stimuli). No difference was found in the magnitude of effect size for verbal and nonverbal stimuli (t(12) = .89, P = .26). Likewise, comparison of dot probe studies using greater than 1,000 msec stimuli presentations (n = 9, d = 0.59) relative to those that used 500 msec presentations (n = 7, d = 0.54) revealed no significant differences (t(12) = .21, P = .85). 刺激类型，呈现时长对结果的发现均没有显著影响 但是,1000的呈现时长肯定是更加灵活的,我可以截一半看早期注意,也可以就着做时程分析,数据量上肯定比500ms的更加丰富,作者由此推1000ms的逻辑不够严谨，一个测量自下而上的注意，一个测量自上而下的注意，差异不显著可能说明两种注意都有，但这结果我感觉是不同的(by 苏师姐),我们觉得要是作者能扯上时程来推1000ms就更好了. There were no significant group differences in attentional bias toward physically threatening stimuli (d = 0.02, n = 3,P = .89, CI = .22, .23); however, there was a trend toward greater attentional bias among depressed participants toward socially threatening stimuli( ≈ 愤怒面孔) (d = 0.15, n = 8, P = .10, CI = .03, .33). 抑郁患者可能对愤怒面孔有注意偏向 In dot probe studies, the effect tended to be larger among patients (n = 6, d = 0.70) relative to nonpatients (n = 8, d = 0.47); however, again this effect was not significant (P = .38). 抑郁患者相比于正常人对dot probe范式的注意偏向效应量会更大 查看抑郁患者的入组标准： Mogg K, Bradley BP, Williams R, Mathews A. Subliminal processing of emotional information in anxiety and depression. J Abnorm Psychol 1993;102:304–311. Mogg K, Millar N, Bradley BP. Biases in eye movements to threatening facial expressions in generalized anxiety disorder and depressive disorder. J Abnorm Psychol 2000;109:695–704. Musa C, Lepine JP, Clark DM, et al. Selective attention in social phobia and the moderating effect of a concurrent depressive disorder. Behav Res Ther 2003;41:1043–4054. Gotlib IH, Krasnoperova E, Yue DN, Joormann J. Attentional biases for negative interpersonal stimuli in clinical depression. J Abnorm Psychol 2004;113:127–135. Gotlib IH, Kasch KL, Traill SK, et al. Cognitive biases and affect persistence in previously dysphoric and never-dysphoric individuals. Cogn Emotion 2004;11:517–538. Joormann J, Gotlib IH. Selective attention to emotional faces following recovery from depression. J Abnorm Psychol 2007; 116:80–85. 本文为反驳Williams的theory of cognitive biases提供了有力证据，证明抑郁患者中也存在焦虑患者中存在的attentional bias，并非只有biased memory提供了证据，并且明确表示dot probe task是比emotional stroop task更好的测量注意偏差的范式，理由是dot probe task的效应量更大且更稳定。Overall, the results of our analysis demonstrate that depressed groups exhibit significantlygreater attentional bias toward negative information relative to nondepressed groups, but the absolute values of this bias (i.e., whether it they are greater than 0) cannot be inferred. And our results verify that attentional bias is present in depression when measured by the dot probe task, a finding that begins to resolve the controversy surrounding attention bias in depression. Williams JMG, Watts FN, MacLeod C, Mathews A. Cognitive Psychology and Emotional Disorders. Chichester, England: Wiley; 1988. Williams JMG, Watts FN, MacLeod C, Mathews A. CognitivePsychology and Emotional Disorders. 2nd ed. Chichester, England: Wiley; 1997.Notably, the absence of a significant difference in biased attention between clinical and nonclinical levels of symptoms was also observed in a meta-analysis of biased attention in anxiety.[14] As such, one implication of our findings is that biased attention to negative stimuli is a state characteristic of negative affect (depressed or anxious mood) rather than a marker of clinical depression. Bar-Haim Y, Lamy D, Pergamin L, et al. Threat-related attentional bias in anxious and nonanxious individuals: a meta-analytic study. Psychol Bull 2007;133:1–24.好的句子： 开篇：Cognitive models of depression assert that biases in the processing of emotional information contribute to the onset and maintenance of the disorder. According to these models, mood-congruent information processing biases influence memory, interpretation, and attention in individuals with emotional disorders. 人对情绪信息的注意偏差可能是由抑郁障碍引起的。 Beck AT. Depression: Clinical, Experimental and Theoretical Aspects. New York: Harper and Row; 1967. Beck AT. Cognitive models of depression. J Cogn Psychother Int Q 1987;1:5–37. The lack of consistent evidence for biased attentional processing in depression has led some researchers to hypothesize that depression is characterized by memory but not attention bias, whereas anxiety is characterized by attentional but not memory bias. “抑郁可能不是由注意偏差影响的”这件事使得人们提出假说：depression serves to focus emotional processing on past failures so that unsuccessful endeavors can be avoided, whereas anxiety helps to protect an individual from current or future threat. MacLeod C, Mathews A, Tata P. Attentional bias in emotional disorders. J Abnorm Psychol 1986;95:15–20. Williams JMG, Watts FN, MacLeod C, Mathews A. Cognitive Psychology and Emotional Disorders. Chichester, England:Wiley; 1988. Williams JMG, Watts FN, MacLeod C, Mathews A. Cognitive Psychology and Emotional Disorders. 2nd ed. Chichester,England: Wiley; 1997. Mathews A, MacLeod C. Cognitive approaches to emotion and emotional disorders. Annu Rev Psychol 1994;45:25–50. Oatley K, Johnson-Laird PN. Towards a cognitive theory of emotions. Cogn Emotion 1987;1:29–50. 关于实验刺激： anxiety患者的注意偏向很明显，然而抑郁患者的不明显，对于如何发现抑郁患者不明显的、subliminal的注意偏向，前人给出了以下经验：刺激至少要呈现1000毫秒（我们的实验有2000毫秒） There is little evidence for subliminal attentional bias in depression, and that an attentional bias in depression is typically only found when the material is self-referent and is presented for long (≥1,000 ms) durations. Mogg K, Bradley BP, Williams R. Attentional bias in anxiety and depression: the role of awareness. Brit J Clin Psychol 1995;34:17–36. There is evidence for biased attention in depression, but only when the information is mood-congruent and presented for a duration of 1,000 msec or more. Mathews A, MacLeod C. Cognitive vulnerability to emotional disorders. Annu Rev Clin Psychol 2005;1:167–195. Threatening stimuli that are often associated with attention bias in anxiety have generally not been associated with biased attention in depression. 如果复现了结果记得要找这篇文章 Mogg K, Millar N, Bradley BP. Biases in eye movements to threatening facial expressions in generalized anxiety disorder and depressive disorder. J Abnorm Psychol 2000;109:695–704. Also, researchers have hypothesized that using emotional faces instead of words may be a better indicator of attentional bias, because faces are both more salient and more strongly related to social interaction than words. 用来做我们使用面孔刺激材料的论据 Gotlib IH, Krasnoperova E, Yue DN, Joormann J. Attentional biases for negative interpersonal stimuli in clinical depression.J Abnorm Psychol 2004;113:127–135. Bradley BP, Mogg K, Millar N, et al. Attentional biases for emotional faces. Cogn Emotion 1997;11:25–42. 关于实验范式:主要有emotional stroop task（对应核磁任务）和dot probe task（对应眼动任务）The emotional Stroop task is a version of the Stroop task that requires participants to report the color of an emotionally valenced word. The emotional Stroop effect is observed when participants with emotional disorders are delayed in their color-naming response to emotional stimuli relative to neutral stimuli.对emotional stroop的批判The emotional Stroop task has been criticized by researchers[1] who argue that it provides an ambiguous measure of attention, that the observed effect may reflect a type of response bias, or that the response is simply the effect of state-dependent mood congruence.批判情绪stroop范式只测量了任务依赖的情绪一致性的判断，而非对情绪本身的注意（核磁的范式是测量的是对情绪面孔和情绪文字的stroop效应，理论上应当是面孔和文字情绪不一致的时候反应时更长，并且正确率更低？）An additional limitation is that the emotional Stroop task is that it allows only one stimulus (one word) to be presented at a time. MacLeod C, Mathews A, Tata P. Attentional bias in emotional disorders. J Abnorm Psychol 1986;95:15–20.In dot probe task, the two stimuli are presented quickly on a computer screen, and one word is replaced by a probe. Participants respond by pressing a key to correspond to the location of the probe. Attentional bias can then be derived by subtracting the average reaction time when the probe replaces negative words from the average reaction time when the probe replaces neutral words.方法：计算效应量According to Cohen’s standards[1], effect sizes (d) of 0.2 are considered small, 0.5 are considered medium, and 0.8 are considered large. Cohen J. Statistical Power Analysis for Behavioral Sciences. 2nd ed. Hillsdale, NJ: Erlbaum; 1988.发表偏差In order to evaluate for publication bias, we calculated the ‘‘failsafe N’’[28,29] separately for emotional Stroop studies and dot probe studies. This method is hypothesized to generate the number of null results needed to render the effect size nonsignificant, and iscalculated by multiplying the number of studies being analyzed by 5 and then adding 10 to this number.[29] Additionally, we constructed a funnel plot based on mean effect size, and evaluated year of publication as a standard moderator to further evaluate the possibility of publication bias. These analyses were completed using the Comprehensive Meta-Analysis software program.[30] Rosenthal R. Meta-Analytic Procedures for Social Research. Newbury Park, CA: Sage; 1991. Rosenthal R, Rubin DB. Comment: assumptions and procedures in the file drawer problem. Stat Sci 1988;3:120–125. Borenstein M, Hedges LV, Higgins JPT, Rothstein HR. 2006. Comprehensive Meta-Analysis (Version 2.2.027). [Computersoftware]. Englewood, NJ: Biostat.不足之处：bias效应没有经过Bootstrap检验。" }, { "title": "npm audit fix报错的解决办法", "url": "/posts/npm-audit-fix%E6%8A%A5%E9%94%99%E7%9A%84%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/", "categories": "随笔", "tags": "Markdown插件", "date": "2022-08-30 12:25:00 +0000", "snippet": "问题提出用npm audit fix 安装包的更新时出现➜ npm auditnpm ERR! code ENOLOCKnpm ERR! audit This command requires an existing lockfile.npm ERR! audit Try creating one first with: npm i --package-lock-onlynpm ERR!...", "content": "问题提出用npm audit fix 安装包的更新时出现➜ npm auditnpm ERR! code ENOLOCKnpm ERR! audit This command requires an existing lockfile.npm ERR! audit Try creating one first with: npm i --package-lock-onlynpm ERR! audit Original error: loadVirtual requires existing shrinkwrap file解决方法参考文章：https://stackoom.com/question/4R8gN#:~:text=npm%20i%20–package-lock-only%20%E5%8F%AA%E7%94%9F%E6%88%90%2F%E6%9B%B4%E6%96%B0,package-lock.json%20%E6%97%A0%E9%9C%80%E9%87%8D%E6%96%B0%E5%AE%89%E8%A3%85%EF%BC%9B%20npm%20i%20%E5%B0%86%E9%87%8D%E6%96%B0%E5%AE%89%E8%A3%85%E5%B9%B6%E7%94%9F%E6%88%90%E4%B8%80%E4%B8%AA%EF%BC%88%E5%9F%BA%E4%BA%8E%E6%82%A8%E7%9A%84%E9%85%8D%E7%BD%AE%EF%BC%89%E3%80%82依次输入npm i --package-lock-onlynpm config get package-locknpm config get shrinkwrapnpm i --package-lock-onlynpm audit fix解决，效果如图我本来想干啥我想装个markdown的html生成插件（链接如下https://github.com/azu/mdline主要是这个时间流的效果很让我心动，然后我先在这里https://nodejs.org/en/download/装了node.js，然后通过npm安装插件：npm install --global mdline然后就出现了这个诡异的更新提示（其实可以不管），但只要你跟着他的提示走，你就会发现开头提出的问题，很烦。于是我浅搜了一下网上的方法，发现放在一个很偏的位置，就贴在这里（没准我的博客更偏，但也算是能搜到吧）" }, { "title": "Word引用文献方法", "url": "/posts/Word%E5%BC%95%E7%94%A8%E6%96%87%E7%8C%AE%E6%96%B9%E6%B3%95/", "categories": "科研", "tags": "Zotero", "date": "2022-08-30 10:50:00 +0000", "snippet": "先把重要的事情说三遍：不要完全依赖软件的引用，要能自己判断引用是否正确！不要完全依赖软件的引用，要能自己判断引用是否正确！不要完全依赖软件的引用，要能自己判断引用是否正确！[引言]Zotero,一款爱不释手的文献管理软件主要参考：https://zhuanlan.zhihu.com/p/30395323Zotero作为一款完全开源的文献管理软件，和许多浏览器有良好的兼容性，可以很方便地下载文...", "content": "先把重要的事情说三遍：不要完全依赖软件的引用，要能自己判断引用是否正确！不要完全依赖软件的引用，要能自己判断引用是否正确！不要完全依赖软件的引用，要能自己判断引用是否正确！[引言]Zotero,一款爱不释手的文献管理软件主要参考：https://zhuanlan.zhihu.com/p/30395323Zotero作为一款完全开源的文献管理软件，和许多浏览器有良好的兼容性，可以很方便地下载文献。作为一款文献管理工具，Zotero既可以帮助文献引用，也可以很方便地帮助管理看文献的笔记。——沃·兹基朔德如何用Zotero在Word中插入参考文献主要参考：https://zhuanlan.zhihu.com/p/62931860当word中菜单栏里没有zotero选项时怎么办：首先，检查zotero里的word插件安装了没先点一下重新安装试试单击重新安装后检查word看插件是否出现如下图所示的zotero栏：如果还不行，可以参考这篇知乎排查word结合Zotero的插件位置是不是没装好：https://zhuanlan.zhihu.com/p/191039036（这篇知乎对更新了win11的同学或者找不到C:\\Users\\用户名\\AppData\\Roaming\\Microsoft\\Word\\STARTUP文件的同学不适用，可能对比较老的电脑有奇效（？））在word中插入引用的步骤大致如下： 单击add\\edit citation 选择apa7(写这篇文档时的最新版APA格式)作为引用格式 如果电脑比较拉容易卡或者引用格式需要手动改的话，就别勾选自动更新引文 在出现Zotero的红框后，可以输入作者名，标题名的关键词搜索引用文献 也可以勾选经典视图选文献 在这zotero界面里面勾选自己想要引用的文献 如果想要在一个地方引用多篇文献，单击底下的多重来源像SPSS那样选择想要加入的文献，每选一篇文献就点一次右箭头加入即可，上下箭头可以调整引用顺序然后就能看到引用的参考文献了在引用完文献后，在文章最后点击add bibliography呈现参考文献目录如下图所示我们不难发现，虽然看上去很接近APA了，然而标题的格式不应该是每个词都有首字母大写，而是只有首字母和冒号后面的字母大写，这种问题往往是由zotero生成条目时使用了首字母大写的方式导致的。我们可以看到，在zotero中，条目是按照首字母自动大写的方式录入的，我们就需要手动修改修改方式很简单，手动改，或者丢到word里划选首字母以外的字母按住shift+F3调整大小写，然后粘回zotero标题处修改。（下图就是个改好的例子）改好后，回到写文章的word，单击refresh更新我们可以看到，word里面的引用就已经更新了如果想要更改正文里面的citation，点击引用的地方修改即可之后点击refresh出现以下警告，单击“是”保存修改之后就可以看到正文中的引用部分修改，而引用区域的参考文献格式保持不变，以上就是word引用的基本技巧" }, { "title": "Zotero插入中文文献方法", "url": "/posts/Zotero%E6%8F%92%E5%85%A5%E4%B8%AD%E6%96%87%E6%96%87%E7%8C%AE/", "categories": "科研", "tags": "Zotero", "date": "2022-08-27 02:34:00 +0000", "snippet": "先把重要的事情说三遍：不要完全依赖软件的引用，要能自己判断引用是否正确！不要完全依赖软件的引用，要能自己判断引用是否正确！不要完全依赖软件的引用，要能自己判断引用是否正确！知网是大家查中文文献经常使用的网站了，这里主要讲如何把知网的文献导入到zotero中并自动生成条目方便在word中引用首先，我们需要安装插件jasminum，其介绍如下：通过打开这个网址点击.xpi超链接来下载https:...", "content": "先把重要的事情说三遍：不要完全依赖软件的引用，要能自己判断引用是否正确！不要完全依赖软件的引用，要能自己判断引用是否正确！不要完全依赖软件的引用，要能自己判断引用是否正确！知网是大家查中文文献经常使用的网站了，这里主要讲如何把知网的文献导入到zotero中并自动生成条目方便在word中引用首先，我们需要安装插件jasminum，其介绍如下：通过打开这个网址点击.xpi超链接来下载https://github.com/l0o0/jasminum确保在你知道的位置下载了这个.xpi之后打开Zotero的插件栏选择install add-on from file选择刚才的.xpi文件，之后重启zoteropdf的导入但是当时是这把pdf拖进去时，你会发现zotero还是无法识别这种情况下你就需要手动改pdf的名字在标题后加_一作的名字之后再点右键抓取知网数据这样就找得到了需要注意的是，在zotero中，中文文献的中文作者名字全部放姓那一栏，因为中文文献的引用不可能只显示作者的姓，所以不要把姓名拆开（英文名作者则需要，如下图温忠麟老先生心理学报文章的英文引用）网页的导入需要知道的是，pdf并非导入文献的唯一方法，我们也可以结合插件在网页上进行zotero文献导入，这里以知网海外版为例知网海外版链接https://chn.oversea.cnki.net/index/首先在上述知网链接里搜文章点开文章，在右上角的zotero插件点一下就能导入(此时你的zotero得是开着的)点了再按一下enter（默认导入到zotero当前开着的文件夹里的）可以看到zotero中已经有了该条目，条目信息和上面是一样的有时候能自动下pdf，有时不能（可能和网络环境有关），如果需要的话就手动下pdf再拖进条目里也是可以的（但只作为引用的话这样也可以了）中文文献的英文引用有同学可能想获取中文文献的英文信息，就比如这种甚至心理学报都要求中文文献要搞英文信息这个东西比较难办，而且也很反人类（文章都是中文的搞啥英文引用）主要给个经验和思路，操作起来还是很麻烦首先，如果想试试白嫖英文引用的话，去引用文献的期刊网站看有没有英文版网站（如心理学报）在里面搜索作者英文名发现所有文章都有英文版通过右上角插件存入，发现可以把条目信息再调整一下（人工检查一遍）这样在Zotero中引用就没问题啦~或者，直接新建条目把pdf最后的英文信息手动整理上来（这条同样可以适用于一些未发表的量表或者文献的引用，如下图）实测后，狠！有效！" }, { "title": "你好！", "url": "/posts/%E4%BD%A0%E5%A5%BD/", "categories": "随笔", "tags": "一切的开始", "date": "2022-08-20 12:55:00 +0000", "snippet": "关于我我是北师大心理学本科的在读学生，平时跟着一位年轻的switch重度玩家院士候选人王老师做科研，喜欢运动，喜欢但不太能打游戏，研究方向是临床心理学结合一点认知和脑，做事比较随心所欲。我很喜欢的一句鸡汤，虽然我的大学并没有《三傻大闹宝莱坞》那样丰富多彩建立这个博客也是很随性的想法：我想记录一下自己在生活中遇到的和学习工作有关的坑（主要是便于自己翻看哈哈），因为我发现有些问题不记录下来过段时...", "content": "关于我我是北师大心理学本科的在读学生，平时跟着一位年轻的switch重度玩家院士候选人王老师做科研，喜欢运动，喜欢但不太能打游戏，研究方向是临床心理学结合一点认知和脑，做事比较随心所欲。我很喜欢的一句鸡汤，虽然我的大学并没有《三傻大闹宝莱坞》那样丰富多彩建立这个博客也是很随性的想法：我想记录一下自己在生活中遇到的和学习工作有关的坑（主要是便于自己翻看哈哈），因为我发现有些问题不记录下来过段时间再遇到可能就忘了，好记性不如烂笔头（特别是编程的东西忘了再找回来简直要命），就抄了B站up主汤姆还在写代码的一个框架https://github.com/tomstillcoding/tomstillcoding.github.io这里体现了互联网精神的重要法则：不要尝试去造别人已经造好的轮子，既然已经有现成的框架，我抄下来用就是，没必要自己写，然后这篇博客就“嗒哒”一下出现啦！后续会更新一下自己以前写过的东西，可能也会整理一下Linux系统里我遇到过的问题，也算是发扬互联网精神吧~博客里留有我的邮箱，如果你想线下找我玩，可以通过邮箱找到我，如果你是这位重度Switch玩家的学生的话，这个博客里可能也会有对你课业有多少帮助的东西，不过我很佛系，没什么世俗的欲望，更新频率就不一定吧，尽量每个月都有一些东西（忙起来就不保证了）。0904更新：我打算每周精读5篇文献，并将他们更新到这个博客的文献类别里，如果缺更的话，可能是因为看不懂，也可能是因为看了没收获，就，大致5天在看文献吧（现在看的文献主要是抑郁的文章，毕设相关，我觉得现在开始看也可以，慢慢来。数据也有空同时处理一下，奴隶做一个合格的学术牛马科研人。" } ]
