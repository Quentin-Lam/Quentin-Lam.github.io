---
title: 如何安装Eyelink眼动实验所需的一切硬件
date: 2023-08-02 13:14:00 +0800
categories: [科研]
tags: [眼动]
pin: false
author: Quentin Lam

toc: true
comments: true
typora-root-url: ../../Quentin-Lam.github.io
math: false
mermaid: true




---

Eyelink 1000 Plus 眼动仪是眼动研究中重要的研究设备。本文将从软件安装，硬件配置，到上手校准3个方面来帮助大家理解眼动实验的每个步骤。



## 1. 软件安装

##### 首先，不管我们是用什么软件进行试验，我们都需要去Eyelink的官网(https://www.sr-research.com/support/)安装所需的组件，网页界面如下：

![image-20230802135626512](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802135626512.png)

##### 注册完注意查收自己的邮箱点击链接完成验证。完成验证后，回到支援网站，会提示你已经验证成功。

<img src="/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802135714852.png" alt="image-20230802135714852" style="zoom:25%;" />

##### 需要注意的是，注册完成后账户不会立马激活，可能要等一天才行。

<img src="/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802135730384.png" alt="image-20230802135730384" style="zoom:50%;" />

##### 激活后会有邮件提示。

<img src="/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802135801729.png" alt="image-20230802135801729" style="zoom: 25%;" />

##### 注册完成后，我们回到下载界面（https://www.sr-research.com/support/），就能看到Eyelink的论坛被解锁啦~

![image-20230802135822126](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802135822126.png)



### 1.1 安装Psychopy

##### 因为我是用python编写的实验，所以这里我选择下载python相关的组件。Psychopy下载地址：https://www.psychopy.org/download.html 。安装完毕后再安装Visual Studio Code：https://code.visualstudio.com/Download 。

![image-20230802140007331](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802140007331.png)

![image-20230802140028603](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802140028603.png)

##### 之后回到Support Forum (https://www.sr-research.com/support/)，选择下载Developer Kit 和 Pylink Download。

##### Pylink Download 可直接输入以下命令：

```c++
py -3.8 -m pip install --index-url=https://pypi.sr-support.com sr-research-pylink
# 安装显卡驱动呈现刺激(视情况安装)
py -3.8 -m pip install pyopengl 
```

（如果安装的是其他版本的python就要把3.8替换为对应的版本数字）



## 2. 实验代码编写

### 2.1 Vscode python环境配置

##### 首先，我们在vscode中安装python拓展。

<img src="/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802142453317.png" alt="image-20230802142453317" style="zoom: 50%;" />

在菜单右下角查看自己的python是否已经载入（我这里显示的版本是3.8.8）

![image-20230802142600672](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802142600672.png)



## 被试机（呈现刺激的电脑）硬件配置：

##### 记得，眼动仪——主试机，被试机——主试机要连两条网线，连好后要将被试机电脑的IP改成100.1.1，不知道怎么改可参考网站链接：

https://www.sr-support.com/showthread.php?tid=281

![image-20230802152429293](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802152429293.png)

### 实验代码：

##### 首先，导入应该要导入的包并建立中文`utf-8`环境

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import sys
# math is for calculating visual angle
from math import pi, atan, hypot
import random
import pylink # we should have installed it before

# This is for using animation for calibration, If not needed, we can comment out it. This isn't a standard package from python, we need to cite where it is for python to find it
sys.path.append('C:/Users/20191/Desktop/ldk/6 yuan/exp_code') # The path where my .py module file is
from EyeLinkCoreGraphicsPsychoPyAnimatedTarget import EyeLinkCoreGraphicsPsychoPy

# import psychopy for stimulation presentation
from psychopy import visual, core, event, monitors
from psychopy.constants import STOPPED, PLAYING
# for preparing the Host backdrop image
from PIL import Image
# 切换输入法，没有这个包的话pip install
import pyautogui
# Specify the font dictionary to override default font paths
# copy your font file to the same directory of your exp file
import pyglet
exp_dir = os.path.dirname(os.path.realpath(__file__))
os.chdir(exp_dir)
font_name = 'kaiu.ttf'
pyglet.font.add_file(os.path.join(exp_dir, font_name))
```

##### 为了方便大家理解，`EyeLinkCoreGraphicsPsychoPyAnimatedTarget`就是这样一个.py文件

<img src="/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230803134114270.png" alt="image-20230803134114270" style="zoom:67%;" />

##### 然后，我们可以输入一些硬件方面的信息（需要根据刺激呈现屏幕调整，需手动测量），放在前面是为了方便以后修改。

```python
# set a few constants
scrWidth = 37.5 # measured in cm
scrHeight = 30

# The resolution of presenting screen
scnSize = [1024, 768] 
pw=scnSize[0] # screen width measured by pixels
ph=scnSize[1]
viewDist = 66 # The viewing distance from chin rest to the screen (cm)
# the formula to calculate visual angles
deg2pix = int((scnSize[0] / 2.0) / (atan(scrWidth / 2.0 / viewDist) * 180.0 / pi))  

# stim size
V_w = 9 # measured in visual angles
V_h = round(V_w * 300 / 260) # make sure that different sizes of stim keeps the same shape
pix_size = (V_w * deg2pix, V_h * deg2pix)

print(f"The the width of the stimulus is {V_w} and the height is {V_h}, and the pixel size of it is {pix_size}")
```

##### 然后，我们可以设置一些眼动仪相关的参数。

```python
# eye-selection global options
RIGHT_EYE = 1
LEFT_EYE = 0
BINOCULAR = 2

# Initialize EyeLink and the Graphics
eyelinktracker = pylink.EyeLink()
if not eyelinktracker:
	print ("EL is None")
	sys.exit()
""" if not dummyMode: 
	tk = pylink.EyeLink('100.1.1.1')
else:
	tk = pylink.EyeLink(None) """
tk = pylink.EyeLink('100.1.1.1')
```

##### 如果眼动仪启动成功，我们便可以开始收集被试信息。

```python
# get subj info
def getSubjInfo():
    pyautogui.keyDown('shift')
	pyautogui.keyUp('shift')
	ID = str(input('Subject ID: '))
    # # You can add and delete whatever you like
	# Name = raw_input('Subject Name: ')
	# Gender = raw_input('Subject Gender: ')
	Age = str(input('Age: '))
	return [ID, Age]



def init_data_file(SUBJ_INFO):
	""" define a function that initializes the data file and get sub_info"""

	if not os.path.exists('csv_data_DotFace'): 
		os.mkdir('csv_data_DotFace')
    # join everything in the SUBJ_INFO seperated by "_"
	file_name = 'csv_data_DotFace/' + "_".join(SUBJ_INFO) + '.csv'
	with open(file_name, 'w') as data_file:
		header = ['subj', 'age', 'trial', 'dot_pos', 'Lstim', 'Rstim', 'stim_st', 'stim_et', 'LoEp', 'cue_pos', 'key_press']
        # this behavorial csv data should contain the info of every trial. Please add or delete things in it if needed
		data_file.write('\t'.join(header) + '\n')
	return data_file

## initialize the data files.
subjInfo = getSubjInfo()
DATA_FILE = init_data_file(subjInfo)
dataFolder = os.getcwd() + '/your_experiment_name/' # CHANGE IT ACCORDING TO YOU EXPERIMENT!
if not os.path.exists(dataFolder): os.makedirs(dataFolder) # create data folder if it's not there
edfFileName = "_".join(subjInfo)  + ".EDF" # create .EDF file for storing eye-tracking data
tk.openDataFile(edfFileName)
```

##### 现在，我们完成了数据文件的创造，我们可以设计实验设计矩阵，这里我们以点探测实验为例建立矩阵。

```python
# prepare stim
script_path  = os.path.abspath(os.path.dirname(__file__))
proj_root = os.sep.join(script_path.split(os.sep)[:-1])

# The trial list needs 5 variables, equals to: 
# cue_pos: the position of fixation point. 1:left, 2:right, 3:center
# LoEp: the location of the emotional picture. 1:left, 2:right
# dot_pos: the position of the pressing dot. 1:left, 2:right 
# stim_e: the picture name of the emotional pictures
# stim_n: the picture name of the neutral pictures
cue_pos = [1, 2, 3]
LoEp = [1, 2]
dot_pos = [1, 2]

# Define functions for generating trial lists
def generate_trial_list(cue_pos, LoEp, dot_pos, stim_e, stim_n):
    trial_list_all = []
    for i in cue_pos:
    	for j in LoEp:
            for k in dot_pos:
                trial_list = []
                for l in range(len(stim_e)):
                    list1 = [i, j, k, stim_e[l], stim_n[l]]
                    trial_list.append(list1)
                trial_list_all.extend(trial_list)
    return trial_list_all

# Get the filenames for emotional and neutral stimuli
d_dir_ExpF = os.path.join(proj_root, 'exp_stim/exp/f/')
d_dir_ExpM = os.path.join(proj_root + 'exp_stim/exp/m/')
d_dir_NF = os.path.join(proj_root, 'exp_stim/N/f/')
d_dir_NM = os.path.join(proj_root, 'exp_stim/N/m/')
d_dir_all = os.path.join(proj_root, 'exp_stim/all/')
file_name_ExpF = os.listdir(d_dir_ExpF)
file_name_ExpM = os.listdir(d_dir_ExpM)
file_name_NF = os.listdir(d_dir_NF)
file_name_NM = os.listdir(d_dir_NM)

file_name_Exp = file_name_ExpF + file_name_ExpM
file_name_N = file_name_NF + file_name_NM
# print(file_name_Exp) # ['FF17.bmp', 'FF23.BMP', 'HF10.BMP', 'HF61.bmp', 'SF16.bmp', 'SF38.BMP', 'FM3.BMP', 'FM36.bmp', 'HM102.bmp', 'HM17.BMP', 'SM18.bmp', 'SM41.BMP']
# Generate the trial lists for different conditions
trial_list_rec = generate_trial_list(cue_pos, LoEp, dot_pos, file_name_Exp, file_name_N)
# print(trial_list_rec[0], len(trial_list_rec)) # [1, 1, 1, 'FF17.bmp', 'NF15.bmp'], 144
```

##### 这个是本点探测范式试次的流程图：

![新变式_英文](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/%E6%96%B0%E5%8F%98%E5%BC%8F_%E8%8B%B1%E6%96%87.png)

##### 以上都设置完了，我们可以初始化屏幕并进入首次校准阶段。

```python
# setup a display
mon = monitors.Monitor('QL', width=scrWidth, distance=viewDist) # name the display with whatever you want
mon.setSizePix((scnSize[0], scnSize[1]))

surf = visual.Window((scnSize[0], scnSize[1]), fullscr=True, monitor=mon, color=[0.5,0.5,0.5], units='pix') # set up full screen and the background color
event.Mouse(visible=False) # hide the mouse

# call the custom calibration routine "EyeLinkCoreGraphicsPsychopy.py", instead of the default routines that were implemented in SDL

# Initiate the custon calibration routine
genv = EyeLinkCoreGraphicsPsychoPy(tk, surf)

# # to use a movie clip as the calibration target
#genv.calTarget = 'movie'
#genv.movieTargetFile = 'starjumps100.avi'

# to use a rotating checkerboard as the calibration target
genv.calTarget = 'rotatingCheckerboard'

# Open the graphics window to show custom calibration pic
pylink.openGraphicsEx(genv)
# # If the previous doesn't works, use it
#pylink.openGraphics()

# Flush keyboard
pylink.flushGetkeyQueue()
# We need to put the tracker in offline mode before we change its configurations
tk.setOfflineMode()
#pylink.setTargetSize(int(0.5*deg2pix), int(0.15*deg2pix))
#pylink.setCalibrationColors(black, gray)

## Gets the display surface and sends a mesage to EDF file;
# set up sample rate
tk.sendCommand('sample_rate 500')
# record the exp pixel coordinates
tk.sendCommand("screen_pixel_coords =  0 0 %d %d" % (pw-1, ph-1))
# sent it to the eye-tracker
tk.sendMessage("DISPLAY_COORDS 0 0 %d %d" % (pw-1, ph-1))
# specify the calibration type, H3, HV3, HV5, HV13 (HV = horiztonal/vertical), 
tk.sendCommand("calibration_type = HV5") # tk.setCalibrationType('HV9') also works, see the Pylink manual http://sr-research.jp/support/manual/EyeLink%20Programmers%20Guide.pdf
# specify the proportion of subject display to calibrate/validate (OPTIONAL, useful for wide screen monitors)
tk.sendCommand("calibration_area_proportion 0.75 0.7")
tk.sendCommand("validation_area_proportion  0.75 0.7")
# Set the tracker to parse Events using "GAZE" (or "HREF") data
#tk.sendCommand("recording_parse_type = GAZE")
# online parsing configuration, 0 for ‘cognitive’, 1 for psychophysical, which is more sensitive to saccade
tk.sendCommand("select_parser_configuration 0")
# set up the viewing dist for eye-tracker
tk.sendCommand(format("simulation_screen_distance {viewDist}"))

# edf contents
tk.sendCommand("file_sample_data  = LEFT,RIGHT,GAZE,AREA,GAZERES,STATUS,HTARGET")
# set link data (used for gaze cursor) 
tk.sendCommand("link_event_filter = LEFT,RIGHT,FIXATION,FIXUPDATE,SACCADE,BLINK,BUTTON")
tk.sendCommand("link_sample_data  = LEFT,RIGHT,GAZE,GAZERES,AREA,STATUS,HTARGET")
```

##### 过完校准，我们就可以开始实验的练习阶段了。我们可以先打包一些基础的刺激绘制函数，方便我们之后呈现刺激。

```python
## pack some basic psychopy function to shorten them
def display_clear():
	""" clear up the screen """
	surf.color=[0.5,0.5,0.5]
	surf.flip()


def put_txt(txt, loc):
	""" Put text on display """
	text_1 = visual.TextStim(surf, text = txt,
	pos = loc,
	color = 'black',
	bold = True)
	text_1.draw()
	surf.flip()
   

def draw_pic(stimuli, pos):
    img = visual.ImageStim(surf, image = stimuli, units = 'pix', size = (V_w, V_h), pos = pos)
    img.draw()
    # we don't flip here in case we need to present more pics at a time
    

def draw_dot(color_arr, pos):
    dot = visual.Circle(surf, radius = deg2pix/2, units = 'pix', fillColor = color_arr, pos = pos)
    dot.draw()
    surf.flip()
    
    
def driftCheck4(fix_pos, fixDur=500, acceptableDev=1*deg2pix):
    '''This is an optional function. It's to make sure that the trial only starts when the eyes gazed at 
    the cue dot for long enough time (>= fixDur). Use draw_dot if you don't need this. ''' 
    # fixDotX, fixDotY are the position of the fixation dot
    event.clearEvents()  # clear cached (keyboard/mouse etc.) events, if there is any
    # present the cue dot
    fix = draw_dot([0, 0, 0], fix_pos)
    fix.draw()
    surf.flip()
    triggered = False
    fixationStartTime = -1
    # checks for the gaze position and calculates the gaze error in pixels from the specified fixation dot position
    while not triggered:
        # check if any new events are available
        dt = tk.getNextData()
        if dt > 0:
            ev = tk.getFloatData()
            if dt == pylink.FIXUPDATE:
                # update the fixation start time
                if fixationStartTime < 0:
                    fixationStartTime = ev.getStartTime()
                                    
                # how much time has elapsed within the current fixation
                fixDuration = ev.getEndTime() - fixationStartTime
                
                # check if the average gaze position
                gazeX, gazeY = ev.getAverageGaze()
                fix.draw()
                # # comment out this if you want to show the gaze position
                # fix111 = visual.Circle(surf, radius=10, units='pix', fillColor=[1, 0, 0], pos=(gazeX - pw / 2, ph / 2 - gazeY))
                # fix111.draw()
                surf.flip()
                gazeError = hypot((gazeX - pw / 2) - fix_pos[0], (ph / 2 - gazeY) - fix_pos[1])
                # trigger if the duration of the fixation is > 300 ms and
                # the gaze position is close to the fixation dot (< 30 pixels)
                if fixDuration >= fixDur and gazeError < acceptableDev:
                    triggered = True
                    #tk.sendCommand('clear_screen 0')  # clear the host screen
        # check for Escape key
        if event.getKeys(keyList=['escape']):
            # If the escape key is pressed, it initiates the eye-tracker setup and drift correction.
            tk.doTrackerSetup()
            #driftCheck(deg2pix, "6.avi")
            #tk.doDriftCorrect(int(fixDotX + pw / 2), int(ph / 2 - fixDotY), 1, 1)
            tk.doDriftCorrect(int(pw / 2), int(ph / 2), 1, 1)
            error = tk.startRecording(1, 1, 1, 1)
            if error: return error
            ## wait for 50 ms to prevent data loss
            pylink.msecDelay(40)
            tk.sendCommand('clear_screen 0')  # clear the host screen
    
def run_a_trial(subjInfo, trial_info, rec):
    # subjInfo include subnum, age .etc, trial_info is the conditions we have for each trial. rec: 0 equals learning phase, 1 equals experiment phase
    if rec == 1: # if it's the experiment phase, we should activate eye-tracker
        # force off-line mode first to prevent eyelink freeze
        tk.setOfflineMode()
        pylink.msecDelay(40) # let pylink delay 40ms to prevent data loss
        # get the info you need for running a trail
    cue_pos, LoEp, dot_pos, stim_e, stim_n = trial_info
    if cue_pos == 1: # cue_dot at the left
        cue_loc=L_rect
    elif cue_pos == 2: # cue_dot at the right
        cue_loc=R_rect
    else: # cue dot at the center
        cue_loc=(0,0)

    if LoEp == 1:
        e_loc = L_rect
        n_loc = R_rect
    else:
        e_loc = R_rect
        n_loc = L_rect

    if dot_pos == 1:
        dot_loc = L_rect
    else:
        dot_loc = R_rect
        
    if rec == 1:
        pylink.msecDelay(40)
        driftCheck4(cue_loc)
        tk.sendMessage("check ok")
        msg = "TRIALadd %d %d %d %d %s" % (Ntrial, cue_pos, LoEp, dot_pos, stim_e)  # The info sent to .edf corresponding to trial num, cue dot position, Location of emotional pic, pressing dot position, and emotional stim name
        img_e = draw_pic(os.path.join(d_dir_all, stim_e), pos = e_loc)
        img_n = draw_pic(os.path.join(d_dir_all, stim_n), pos = n_loc)
    else: # for the learning phase
        draw_dot([0, 0, 0], cue_loc)
        core.wait(2)
        draw_pic(os.path.join(d_dir_learn, stim_e), pos = e_loc)
        draw_pic(os.path.join(d_dir_learn, stim_n), pos = n_loc)
	surf.flip()
    if rec == 1:
        img_st = tk.trackerTime()
    core.wait(2.5)
    if rec == 1:
        tk.sendMessage("trial ok1")
        dot_st = core.getTime()
    draw_dot([0, 1, 0], dot_loc)
    resp = event.waitKeys(200000, key_list, timeStamped=True)
	if rec == 1:
        trial_data = subjInfo + [Ntrial] + trial_info + [img_st] + [dot_st] + list(resp[0])
    elif rec == 0:
        trial_data = trial_info + list(resp[0])
	if (trial_info[2] == 1 and resp[0][0] == key_list[0]) | (trial_info[2] == 2 and resp[0][0] == key_list[1]):
        trial_data += [1] 
    else:
        trial_data += [0]
	if rec == 0:
        learn_data.append(trial_data[-1])
```

##### 建立练习阶段：

```python
Ntrial = 0
# This is for learning phase
d_dir_learn = os.path.join(proj_root, 'exp_stim/prac')
stim_p1 = ['1.bmp']
stim_p2 = ['2.bmp']
LoEp_L = [1] # 两张练习图片是同质的我们就不换位置了
learnlist = generate_trial_list(cue_pos, LoEp_L, dot_pos, stim_p1, stim_p2)

# Here the L_rect means that the left pic goes 9 vis ang left away from the center of the screen
L_rect=(-deg2pix * 9, 0) # lr_pos=1, rep left
R_rect=(deg2pix * 9, 0)

def learn_phase():
    # the loc (0,0) means the center of the screen
	put_txt(u"""現在請您練習下。 
            \n\n 
            白色小圓圈出現時，請看它。 
            \n\n 
            之後，兩張面孔出現，請自由觀察面孔。 
            \n\n 
            面孔消失後會出現綠色圓圈，請判斷在屏幕左邊還是右邊
            \n\n
            如果在左邊請按 z 鍵，在右邊按 / 鍵。 
            \n\n 
            明白後按空格鍵開始""", (0,0))
    event.waitKeys(keyList =['space'])
    global key_list
    key_list = ['z', 'slash']
    display_clear()
    random.shuffle(learnlist)
    global learn_data
    learn_data = []
    for t1 in learnlist[:]:
        global Ntrial
		Ntrial = Ntrial + 1
		run_a_trial(subjInfo, t1, 0)
		display_clear()
        # the ITI is randomized between 1000 and 1400 ms
		core.wait(random.uniform(1, 1.4))    
learn_phase()
acc = sum(learn_data) / len(learnlist) * 100
put_txt(f"很棒，你現在已經完成了練習階段，你的正確率是: \n\n {acc} %", (0, 0))
core.wait(1.5)
```

