---
title: 如何安装Eyelink眼动实验所需的一切硬件
date: 2023-08-02 13:14:00 +0800
categories: [科研]
tags: [眼动]
pin: false
author: Quentin Lam

toc: true
comments: true
typora-root-url: ../../Quentin-Lam.github.io
math: false
mermaid: true




---

Eyelink 1000 Plus 眼动仪是眼动研究中重要的研究设备。本文将从软件安装，硬件配置，到上手校准3个方面来帮助大家理解眼动实验的每个步骤。



## 1. 软件安装

##### 首先，不管我们是用什么软件进行试验，我们都需要去Eyelink的官网(https://www.sr-research.com/support/)安装所需的组件，网页界面如下：

![image-20230802135626512](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802135626512.png)

##### 注册完注意查收自己的邮箱点击链接完成验证。完成验证后，回到支援网站，会提示你已经验证成功。

<img src="/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802135714852.png" alt="image-20230802135714852" style="zoom:25%;" />

##### 需要注意的是，注册完成后账户不会立马激活，可能要等一天才行。

<img src="/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802135730384.png" alt="image-20230802135730384" style="zoom:50%;" />

##### 激活后会有邮件提示。

<img src="/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802135801729.png" alt="image-20230802135801729" style="zoom: 25%;" />

##### 注册完成后，我们回到下载界面（https://www.sr-research.com/support/），就能看到Eyelink的论坛被解锁啦~

![image-20230802135822126](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802135822126.png)



### 1.1 安装Psychopy

##### 因为我是用python编写的实验，所以这里我选择下载python相关的组件。Psychopy下载地址：https://www.psychopy.org/download.html 。安装完毕后再安装Visual Studio Code：https://code.visualstudio.com/Download 。

![image-20230802140007331](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802140007331.png)

![image-20230802140028603](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802140028603.png)

##### 之后回到Support Forum (https://www.sr-research.com/support/)，选择下载Developer Kit 和 Pylink Download。

##### Pylink Download 可直接输入以下命令：

```c++
py -3.8 -m pip install --index-url=https://pypi.sr-support.com sr-research-pylink
```

（如果安装的是其他版本的python就要把3.8替换为对应的版本数字）



## 2. 实验代码编写

### 2.1 Vscode python环境配置

##### 首先，我们在vscode中安装python拓展。

<img src="/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802142453317.png" alt="image-20230802142453317" style="zoom: 50%;" />

在菜单右下角查看自己的python是否已经载入（我这里显示的版本是3.8.8）

![image-20230802142600672](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802142600672.png)



## 被试机（呈现刺激的电脑）硬件配置：

##### 记得，眼动仪——主试机，被试机——主试机要连两条网线，连好后要将被试机电脑的IP改成100.1.1，不知道怎么改可参考网站链接：

https://www.sr-support.com/showthread.php?tid=281

![image-20230802152429293](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230802152429293.png)

### 实验代码：

##### 首先，导入应该要导入的包并建立中文`utf-8`环境

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import sys
# math is for calculating visual angle
from math import pi, atan, hypot
import random
import pylink # we should have installed it before

# This is for using animation for calibration, If not needed, we can comment out it. This isn't a standard package from python, we need to cite where it is for python to find it
sys.path.append('C:/Users/20191/Desktop/ldk/6 yuan/exp_code') # The path where my .py module file is
from EyeLinkCoreGraphicsPsychoPyAnimatedTarget import EyeLinkCoreGraphicsPsychoPy

# import psychopy for stimulation presentation
from psychopy import visual, core, event, monitors
from psychopy.constants import STOPPED, PLAYING
# for preparing the Host backdrop image
from PIL import Image

# Specify the font dictionary to override default font paths
# copy your font file to the same directory of your exp file
import pyglet
exp_dir = os.path.dirname(os.path.realpath(__file__))
os.chdir(exp_dir)
font_name = 'SourceHanSansSC-Bold-2.otf'
pyglet.font.add_file(os.path.join(exp_dir, font_name))
```

##### 为了方便大家理解，`EyeLinkCoreGraphicsPsychoPyAnimatedTarget`就是这样一个.py文件

<img src="/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/image-20230803134114270.png" alt="image-20230803134114270" style="zoom:67%;" />

##### 然后，我们可以输入一些硬件方面的信息（需要根据刺激呈现屏幕调整，需手动测量），放在前面是为了方便以后修改。

```python
# set a few constants
scrWidth = 37.5 # measured in cm
scrHeight = 30

# The resolution of presenting screen
scnSize = [1024, 768] 
pw=scnSize[0] # screen width measured by pixels
ph=scnSize[1]
viewDist = 66 # The viewing distance from chin rest to the screen (cm)
# the formula to calculate visual angles
deg2pix = int((scnSize[0] / 2.0) / (atan(scrWidth / 2.0 / viewDist) * 180.0 / pi))  

# stim size
V_w = 9 # measured in visual angles
V_h = round(V_h * 300 / 260) # make sure that different sizes of stim keeps the same shape
pix_size = (V_w * deg2pix, V_h * deg2pix)

print(f{"The the width of the stimulus is {V_w} and the height is {V_h}, and the pixel size of it is {pix_size}"})
```

##### 然后，我们可以设置一些眼动仪相关的参数。

```python
# eye-selection global options
RIGHT_EYE = 1
LEFT_EYE = 0
BINOCULAR = 2

# Initialize EyeLink and the Graphics
eyelinktracker = pylink.EyeLink()
if not eyelinktracker:
	print ("EL is None")
	sys.exit()
""" if not dummyMode: 
	tk = pylink.EyeLink('100.1.1.1')
else:
	tk = pylink.EyeLink(None) """
tk = pylink.EyeLink('100.1.1.1')
```

##### 如果眼动仪启动成功，我们便可以开始收集被试信息。

```python
# get subj info
def getSubjInfo():
	ID = str(input('Subject ID: '))
    # # You can add and delete whatever you like
	# Name = raw_input('Subject Name: ')
	# Gender = raw_input('Subject Gender: ')
	Age = str(input('Age: '))
	return [ID, Age]



def init_data_file(SUBJ_INFO):
	""" define a function that initializes the data file and get sub_info"""

	if not os.path.exists('csv_data_DotFace'): 
		os.mkdir('csv_data_DotFace')
    # join everything in the SUBJ_INFO seperated by "_"
	file_name = 'csv_data_DotFace/' + "_".join(SUBJ_INFO) + '.csv'
	with open(file_name, 'w') as data_file:
		header = ['subj', 'age', 'trial', 'dot_pos', 'Lstim', 'Rstim', 'stim_st', 'stim_et', 'LorR', 'cue_pos', 'key_press']
        # this behavorial csv data should contain the info of every trial. Please add or delete things in it if needed
		data_file.write('\t'.join(header) + '\n')
	return data_file

## initialize the data files.
subjInfo = getSubjInfo()
DATA_FILE = init_data_file(subjInfo)
dataFolder = os.getcwd() + '/your_experiment_name/' # CHANGE IT ACCORDING TO YOU EXPERIMENT!
if not os.path.exists(dataFolder): os.makedirs(dataFolder) # create data folder if it's not there
edfFileName = "_".join(subjInfo)  + ".EDF" # create .EDF file for storing eye-tracking data
tk.openDataFile(edfFileName)


```

##### 现在，我们完成了数据文件的创造，我们可以设计实验设计矩阵，这里我们以点探测实验为例建立矩阵。

```python
# prepare stim
script_path  = os.path.abspath(os.path.dirname(__file__))
proj_root = os.sep.join(script_path.split(os.sep)[:-1])
# Here the L_rect means that the left pic goes 9 vis ang left away from the center of the screen
L_rect=(-deg2pix * 9, 0) # lr_pos=1, rep left
R_rect=(deg2pix * 9, 0)

# get the trial list ready here
# These are the stim images we need to load
d_dir_ExpF = proj_root + '/exp_stim/exp/f/'
d_dir_ExpM = proj_root + '/exp_stim/exp/m/'
d_dir_NF = proj_root + '/exp_stim/N/f/'
d_dir_NM = proj_root + '/exp_stim/N/m/'
d_dir_all = proj_root + '/exp_stim/all/'
file_name_ExpF = os.listdir(d_dir_ExpF)
file_name_ExpM = os.listdir(d_dir_ExpM)
file_name_NF = os.listdir(d_dir_NF)
file_name_NM = os.listdir(d_dir_NM)
file_name_Exp = file_name_ExpF + file_name_ExpM
file_name_N = file_name_NF + file_name_NM

# this is the trial list of all trials in all conditions
triallist_2=[]

# We are giving different conditions different marks here
# 情绪左边，引导左边。LorR=1 & cue_pos=1 
triallist_2_1 = []  
for i in range(len(file_name_Exp)):
	list1 = [1, 1, 1, file_name_Exp[i], file_name_N[i]]
	triallist_2_1.append(list1)

# 情绪右边，引导左边。LorR=2 & cue_pos=1
triallist_2_2 = []  
for i in range(len(file_name_Exp)):
	list1 = [1, 2, 1, file_name_Exp[i], file_name_N[i]]
	triallist_2_2.append(list1)

# 情绪左边，引导右边。LorR=1 & cue_pos=2 
triallist_2_3 = []  
for i in range(len(file_name_Exp)):
	list1 = [1, 1, 2, file_name_Exp[i], file_name_N[i]]
	triallist_2_3.append(list1)

# 情绪右边，引导右边。LorR=2 & cue_pos=2 
triallist_2_4 = []  
for i in range(len(file_name_Exp)):
	list1 = [1, 2, 2, file_name_Exp[i], file_name_N[i]]
	triallist_2_4.append(list1)

# 情绪左边，引导中间边。LorR=1 & cue_pos=3 
triallist_2_5 = []  
for i in range(len(file_name_Exp)):
	list1 = [1, 1, 3, file_name_Exp[i], file_name_N[i]]
	triallist_2_5.append(list1)

# 情绪右边，引导中间。LorR=2 & cue_pos=3     
triallist_2_6 = []  
for i in range(len(file_name_Exp)):
	list1 = [1, 2, 3, file_name_Exp[i], file_name_N[i]]
	triallist_2_6.append(list1)

triallist_2=triallist_2_1+triallist_2_2+triallist_2_3+triallist_2_4+triallist_2_5+triallist_2_6
# print(triallist_2)
for i in range(len(triallist_2)):
	list1 = [2]+triallist_2[i][1:]
	triallist_2.append(list1)

# This is for learning phase
triallist = [[1, 1,'z'], [1, 2,'z'], [1, 3,'z'], [2, 1, 'slash'], [2, 2, 'slash'], [2, 3, 'slash']]
stim_p1 = proj_root + '/exp_stim/prac/1.bmp'
stim_p2 = proj_root + '/exp_stim/prac/2.bmp'


```

##### 这个是本点探测范式试次的流程图：

![新变式_英文](/assets/blog_res/2023-08-02-%E5%A6%82%E4%BD%95%E5%AE%89%E8%A3%85Eyelink%E7%9C%BC%E5%8A%A8%E5%AE%9E%E9%AA%8C%E6%89%80%E9%9C%80%E7%9A%84%E4%B8%80%E5%88%87%E7%A1%AC%E4%BB%B6.assets/%E6%96%B0%E5%8F%98%E5%BC%8F_%E8%8B%B1%E6%96%87.png)

##### 以上都设置完了，我们可以初始化屏幕并进入首次校准阶段。

```python
# setup a display

mon = monitors.Monitor('QL', width=scrWidth, distance=viewDist) # name the display with whatever you want
mon.setSizePix((scnSize[0], scnSize[1]))

surf = visual.Window((scnSize[0], scnSize[1]), fullscr=True, monitor=mon, color=[0.5,0.5,0.5], units='pix') # set up full screen and the background color
event.Mouse(visible=False) # hide the mouse

# call the custom calibration routine "EyeLinkCoreGraphicsPsychopy.py", instead of the default routines that were implemented in SDL

# Initiate the custon calibration routine
genv = EyeLinkCoreGraphicsPsychoPy(tk, surf)

# # to use a movie clip as the calibration target
#genv.calTarget = 'movie'
#genv.movieTargetFile = 'starjumps100.avi'

# to use a rotating checkerboard as the calibration target
genv.calTarget = 'rotatingCheckerboard'

# Open the graphics window to show custom calibration pic
pylink.openGraphicsEx(genv)
# # If the previous doesn't works, use it
#pylink.openGraphics()

# Flush keyboard
pylink.flushGetkeyQueue()
# We need to put the tracker in offline mode before we change its configurations
tk.setOfflineMode()
#pylink.setTargetSize(int(0.5*deg2pix), int(0.15*deg2pix))
#pylink.setCalibrationColors(black, gray)

## Gets the display surface and sends a mesage to EDF file;
# set up sample rate
tk.sendCommand('sample_rate 500')
# record the exp pixel coordinates
tk.sendCommand("screen_pixel_coords =  0 0 %d %d" % (pw-1, ph-1))
# sent it to the eye-tracker
tk.sendMessage("DISPLAY_COORDS 0 0 %d %d" % (pw-1, ph-1))
# specify the calibration type, H3, HV3, HV5, HV13 (HV = horiztonal/vertical), 
tk.sendCommand("calibration_type = HV5") # tk.setCalibrationType('HV9') also works, see the Pylink manual http://sr-research.jp/support/manual/EyeLink%20Programmers%20Guide.pdf
# specify the proportion of subject display to calibrate/validate (OPTIONAL, useful for wide screen monitors)
tk.sendCommand("calibration_area_proportion 0.75 0.7")
tk.sendCommand("validation_area_proportion  0.75 0.7")
# Set the tracker to parse Events using "GAZE" (or "HREF") data
#tk.sendCommand("recording_parse_type = GAZE")
# online parsing configuration, 0 for ‘cognitive’, 1 for psychophysical, which is more sensitive to saccade
tk.sendCommand("select_parser_configuration 0")
# set up the viewing dist for eye-tracker
tk.sendCommand(format("simulation_screen_distance {viewDist}"))

# edf contents
tk.sendCommand("file_sample_data  = LEFT,RIGHT,GAZE,AREA,GAZERES,STATUS,HTARGET")
# set link data (used for gaze cursor) 
tk.sendCommand("link_event_filter = LEFT,RIGHT,FIXATION,FIXUPDATE,SACCADE,BLINK,BUTTON")
tk.sendCommand("link_sample_data  = LEFT,RIGHT,GAZE,GAZERES,AREA,STATUS,HTARGET")
```

##### 过完校准，我们就可以开始实验的练习阶段了。我们可以先打包一些基础的刺激绘制函数，方便我们之后呈现刺激。

```python
## pack some basic psychopy function to shorten them
def display_clear():
	""" clear up the screen """
	surf.color=[0.5,0.5,0.5]
	surf.flip()


def put_txt(txt, loc):
	""" Put text on display """
	text_1 = visual.TextStim(surf, text = txt,
	pos = loc,
	color = 'black',
	bold = True)
	text_1.draw()
	surf.flip()
   

def drawPic(stimuli, pos):
    img = visual.ImageStim(surf, image = stimuli, units = 'pix', size = (V_w, V_h), pos = pos)
    img.draw()
    surf.flip()
    
    
```

